# ╔══════════════════════════════════════════════════════════╗
# ║  MindFlayer v2.0.0 — Environment Configuration          ║
# ║                                                          ║
# ║  Copy this file to .env and fill in your values:         ║
# ║    cp .env.example .env                                  ║
# ║                                                          ║
# ║  IMPORTANT: Never commit .env to version control!        ║
# ║  For production, use a secrets manager instead.          ║
# ╚══════════════════════════════════════════════════════════╝

# ── LLM Provider ──────────────────────────────────────────
# Which provider to use: openrouter | ollama | vllm | tgi | azure
LLM_PROVIDER=openrouter

# ── OpenRouter (Cloud) ────────────────────────────────────
# Get your key: https://openrouter.ai/keys
OPENROUTER_API_KEY=
OPENROUTER_BASE_URL=https://openrouter.ai/api/v1

# ── Azure OpenAI (Cloud) ─────────────────────────────────
# Azure portal → your OpenAI resource → Keys and Endpoint
AZURE_ENDPOINT=
AZURE_API_KEY=
AZURE_API_VERSION=2024-02-01
AZURE_DEPLOYMENT_PARSING=
AZURE_DEPLOYMENT_GENERATION=

# ── Ollama (Local) ────────────────────────────────────────
# Default: http://localhost:11434
OLLAMA_BASE_URL=http://localhost:11434

# ── vLLM (Local) ──────────────────────────────────────────
# Default: http://localhost:8001
VLLM_BASE_URL=http://localhost:8001

# ── HuggingFace TGI (Local) ──────────────────────────────
# Default: http://localhost:8080
TGI_BASE_URL=http://localhost:8080

# ── AI Models ─────────────────────────────────────────────
# Model used for parsing natural language → structured endpoints
PARSING_MODEL=google/gemini-2.0-flash-001

# Model used for generating test code
GENERATION_MODEL=deepseek/deepseek-chat-v3-0324:free

# ── LLM Parameters ───────────────────────────────────────
PARSING_TEMPERATURE=0.3
GENERATION_TEMPERATURE=0.4

# ── Retry Strategy ────────────────────────────────────────
LLM_MAX_RETRIES=3
LLM_RETRY_BASE_DELAY=1.0

# ── Security / Data Privacy ──────────────────────────────
# Set to false to block ALL external API calls (OpenRouter, Azure)
# Only local providers (Ollama, vLLM, TGI) will be allowed
ALLOW_EXTERNAL_CALLS=true

# ── Server ────────────────────────────────────────────────
PORT=8000
CORS_ORIGINS=["http://localhost:3000","http://localhost:3001"]

# ╔══════════════════════════════════════════════════════════╗
# ║  FUTURE: Secrets Manager Integration                     ║
# ║                                                          ║
# ║  For production deployments, replace .env with:          ║
# ║  • Kubernetes Secrets  (k8s volumeMount)                 ║
# ║  • AWS Secrets Manager (boto3 + rotation)                ║
# ║  • Azure Key Vault     (azure-identity SDK)              ║
# ║  • HashiCorp Vault     (hvac client)                     ║
# ║  • Google Secret Manager                                 ║
# ║                                                          ║
# ║  The secrets_provider.py module will abstract this       ║
# ║  so config.py stays unchanged regardless of backend.     ║
# ╚══════════════════════════════════════════════════════════╝
